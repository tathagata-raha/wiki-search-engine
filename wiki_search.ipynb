{"cells":[{"cell_type":"markdown","source":"Importing the required libraries","metadata":{"tags":[],"cell_id":"00000-32c31f85-b354-440f-9bee-07bf17bcf85d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-2aa92249-39ce-457a-92d0-289dc06fb126"},"source":"import re\nimport pickle\nimport Stemmer\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\nimport sys\nimport os","execution_count":539,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we have instantiated some global variables, initiated the stemmer and read the index file","metadata":{"tags":[],"cell_id":"00002-ab0f0b15-23a9-4a80-bd10-fad94b965e57"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-2c87213f-5fea-4387-affd-c95a73482b61"},"source":"STOP_DICT = {}\n# INPUT_FILE = '../enwiki-20200801-pages-articles-multistream1.xml-p1p30303'\nINDEX_DIR = 'index/'\nQUERY = 't:world cup'\nif INDEX_DIR[-1]!='/':\n    INDEX_DIR+='/'\nSTOP_DICT = {}\nSTOP_FILE = ''\nif INDEX_DIR.split('/')[0] == '2018114017':\n    STOP_FILE = '2018114017/frequent.pickle'\nelse:\n    STOP_FILE = 'frequent.pickle'\nwith open(STOP_FILE, 'rb') as handle:\n    STOP_DICT = pickle.load(handle)\nhandle.close()\nstemmer = Stemmer.Stemmer('english')\n# f = open(INDEX_DIR+\"index\", \"r\")\n# index_string = f.read()\n# f.close()\n# temp = index_string.split('\\n')\n# index = defaultdict(list)\n# for i in temp:\n#     splits = i.split(':')\n#     index[splits[0]] = splits[1].split(' ')","execution_count":540,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-7d26b4ea-94bc-4411-aba0-4fb66694bdae"},"source":"len(STOP_DICT.keys())","execution_count":541,"outputs":[{"output_type":"execute_result","execution_count":541,"data":{"text/plain":"660"},"metadata":{}}]},{"cell_type":"markdown","source":"The preprocess function is to process the text. It tokenizes the data, removes unnecessary \nnon-ASCII characters and punctuations, stem the words using pystemmer and remove stop words","metadata":{"tags":[],"cell_id":"00004-8864224f-1b91-432e-adc8-2e6c79926186"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-410a6b96-03eb-4a0b-af70-49b0de07d865"},"source":"def preprocess(text):\n    text = text.lower()\n    tokens = re.sub(r'[^A-Za-z0-9]+', r' ', text).split()\n    stemmed_stop_free = []\n    for token in tokens:\n        if token not in STOP_DICT:\n            stemmed_stop_free.append(stemmer.stemWord(token))\n    return stemmed_stop_free","execution_count":542,"outputs":[]},{"cell_type":"markdown","source":"The parse_query function parses the function and determines if it is a field query or a normal query","metadata":{"tags":[],"cell_id":"00006-da3ca957-df73-4fea-8800-27c8a63d3ae3"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-00340e3c-ef61-4674-acf6-a132dd83098c"},"source":"def parse_query(query):\n    query_list = query.split(':')\n    if len(query_list) == 1:\n        return preprocess(query_list[0]) , 0\n    else:\n        query_dict = {}\n        # query_dict[query_list[0]] = ''\n        for i in range(1, len(query_list)-1):\n            query_dict[query_list[i-1][-1]] = preprocess(query_list[i][:-2])\n        query_dict[query_list[-2][-1]] = preprocess(query_list[-1])\n        return query_dict, 1","execution_count":543,"outputs":[]},{"cell_type":"markdown","source":"This function is used to handled to handle the normal query","metadata":{"tags":[],"cell_id":"00008-318fa041-8470-4717-9581-a2740acb36a7"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-7a57b33a-3c84-4e6e-99ec-32db27714564"},"source":"def search(token,f):\n    # Compute filesize of open file sent to us\n    global indexsize\n    indexsize = os.fstat(f.fileno()).st_size\n    hi = indexsize\n    lo=0\n    x=token\n    while lo < hi:\n        mid = (lo+hi)//2\n        f.seek(mid)\n        while f.read(1) != '\\n':\n            pass\n        line = f.readline()\n        row=line.split(':')\n        midval = row[0]\n        if midval < x:\n            lo=mid\n        elif midval > x: \n            hi=mid\n        else:\n            offsetval = int(row[1])\n            ind = open('./index/index','r')\n            ind.seek(offsetval)\n            line = ind.readline()\n            post = line.split(':')[1]\n            return post\n    return False","execution_count":544,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-69ac3b9b-19d0-4ca6-908b-4234e812d0fc","allow_embed":false},"source":"# def search(token,delim,f):\n#     # Compute filesize of open file sent to us\n#     global indexsize\n#     indexsize = 0\n\n#     loc = token\n\n#     if indexsize == 0:\n#         indexsize = os.fstat(f.fileno()).st_size\n#     hi = indexsize\n\n#     lo=0\n#     lookfor=loc\n#     # print \"looking for: \",lookfor\n#     while hi-lo > 1:\n#         # Find midpoint and seek to it\n#         loc = int((hi+lo)/2)\n#         # print(\" hi = \",hi,\" lo = \",lo)\n#         # print \"seek to: \",loc\n#         f.seek(loc)\n#         # Skip to beginning of line\n#         while f.read(1) != '\\n':\n#             pass\n\n#         line = f.readline()\n#         row=line.split(delim)\n\n#         s = row[0]\n#         if delim == ' ':\n#             s=int(row[0])\n\n#         # post=row[1]\n#         # print(s)\n#         # print(lookfor>s)\n\n#         if lookfor == s:\n#             # print(\"Found: \",lookfor)\n#             if delim == ':':\n#                 offsetval = int(row[1])\n#                 ind = open('./index/index','r')\n#                 ind.seek(offsetval)\n#                 line = ind.readline()\n            \n#                 post = line.split(':')[1]\n            \n#             elif delim == ' ':\n#                 post = ''\n#                 for i in range(1,len(row)):\n#                     post += row[i]+' '\n\n#             f.close()\n#             # print(\"Found: \",lookfor)\n#             return post  # Found\n\n#         if lookfor < s:\n#             # print('h1')\n#             # Split into lower half\n#             hi=loc\n\n#         if lookfor > s:\n#             # print('h2')\n#             # Split into higher half\n#             lo=loc\n        \n#     # If not found\n#     # print(\"Not Found: \",lookfor)\n#     f.close()\n#     return False","execution_count":545,"outputs":[]},{"cell_type":"code","source":"def search_titles(token,f):\n    # Compute filesize of open file sent to us\n    global indexsize\n    indexsize = os.fstat(f.fileno()).st_size\n    hi = indexsize\n    lo=0\n    x=token\n    while lo < hi:\n        mid = (lo+hi)//2\n        f.seek(mid)\n        while f.read(1) != '\\n':\n            pass\n        line = f.readline()\n        row=line.split(':')\n        midval = int(row[0].strip())\n        if midval < x:\n            lo=mid\n        elif midval > x: \n            hi=mid\n        else:\n            post = ''\n            for i in range(1,len(row)):\n                post += row[i]+' '\n            f.close()\n            return post.strip()\n    return False\n# def search_titles(token,f):\n#     global indexsize\n#     indexsize = 0\n\n#     loc = token\n\n#     if indexsize == 0:\n#         indexsize = os.fstat(f.fileno()).st_size\n#     hi = indexsize\n\n#     lo=0\n#     lookfor=loc\n#     # print \"looking for: \",lookfor\n#     while hi-lo > 1:\n#         # Find midpoint and seek to it\n#         loc = int((hi+lo)/2)\n#         # print(\" hi = \",hi,\" lo = \",lo)\n#         # print \"seek to: \",loc\n#         f.seek(loc)\n#         # Skip to beginning of line\n#         while f.read(1) != '\\n':\n#             pass\n\n#         line = f.readline()\n#         row=line.split(':')\n#         s=int(row[0].strip())\n\n#         # post=row[1]\n#         # print(s)\n#         # print(lookfor>s)\n\n#         if lookfor == s:\n#             # print(\"Found: \",lookfor)\n#             post = ''\n#             for i in range(1,len(row)):\n#                 post += row[i]+' '\n            \n#             f.close()\n#             # print(\"Found: \",lookfor)\n#             return post.strip()  # Found\n\n#         if lookfor < s:\n#             # print('h1')\n#             # Split into lower half\n#             hi=loc\n\n#         if lookfor > s:\n#             # print('h2')\n#             # Split into higher half\n#             lo=loc\n        \n#     # If not found\n#     # print(\"Not Found: \",lookfor)\n#     f.close()\n#     return False","metadata":{"tags":[],"cell_id":"00011-37c7c883-2e8c-4c2a-896e-fee5d5121e3b"},"outputs":[],"execution_count":546},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-98071860-ad73-4893-99ef-5f53bdaa8f25"},"source":"def fieldsplit(listing):\n    listingdict = {}\n    fields = ['t', 'b', 'i', 'c', 'r', 'l','d']\n    postinglist = re.findall('[a-z]|[0-9]{1,1000}',listing)\n    # print(postinglist)\n    for item in range(len(postinglist)):\n        cur = postinglist[item]\n        if cur in fields:\n            listingdict[cur] = int(postinglist[item+1])\n    # print(listingdict)\n    return listingdict","execution_count":547,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-72940552-1ad9-4b2e-a10d-4c6b1a4ba17e"},"source":"def find_string_whole(tokens):\n    query_listings = {}\n    for token in tokens:\n        query_listing = search(token,f = open('index/offset','r')).strip().split(' ')\n        for i in range(len(query_listing)):\n            listing = query_listing[i]\n            listingdict = fieldsplit(listing)\n\n            query_listing[i] = listingdict\n        \n        query_listings[token] = query_listing\n    return query_listings\n    #     for i in range(len(query_listing)):\n    #         listing = query_listing[i]\n    #         listingdict = self.fieldsplit(listing)\n\n    #         query_listing[i] = listingdict\n        \n    #     query_listings[token] = query_listing\n\n    # return self.get_matches(query_listings)","execution_count":548,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-82cd15de-b97f-4578-8d23-d6efa71f825b"},"source":"def rank(docs):\n    chosen_keys = []\n    for key,val in docs[:10]:\n        # fp = open(\"./index/titles\",'r')\n        # print(key)\n        # docs = self.search(key,' ',fp)\n        chosen_keys.append(key)\n        # fp.close()\n    return chosen_keys","execution_count":549,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-54e1dd12-00bd-48cc-a8c4-f0214abad0d2"},"source":"def get_matches(query_listings):\n    results = defaultdict(lambda: 0)\n    docs = defaultdict(lambda: 1)\n    weight = {'t':100000, 'b':50, 'i':25, 'c':10, 'r':10, 'l':10}\n\n    for token in query_listings:\n        for query_listing in query_listings[token]:\n            # print(query_listing)\n            for key in query_listing:\n                if key == 'd':\n                    docs[query_listing['d']] *= 2\n                elif key != 'd':\n                    if key == 't':\n                        # print('weight,100000: ',query_listing['d'])\n                        results[query_listing['d']] += query_listing[key]*docs[query_listing['d']]*weight[key]\n\n    sorteddict = [(k, results[k]) for k in sorted(results, key=results.get, reverse=True)]\n    # print(sorteddict)\n    # print('\\n')\n    return sorteddict","execution_count":550,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-e0b70827-c404-4002-b095-10befa4f6a0c"},"source":"def print_titles(chosen):\n    for i in chosen:\n        fp = open(\"./index/titles\",'r')\n        docs = search_titles(i,fp)\n        print(docs)","execution_count":551,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-8391eca1-d76e-4546-89b3-a668ef303b3c"},"source":"def run_whole_query(query):\n    docs_intersect = []\n    docs_postlist = find_string_whole(query)\n    matches = get_matches(docs_postlist)\n    rank(matches)\n    return docs_intersect, docs_postlist","execution_count":552,"outputs":[]},{"cell_type":"markdown","source":"This function is used to handled to handle the field query","metadata":{"tags":[],"cell_id":"00010-edd2a198-23aa-4691-82ad-b7bf4e0fc306"}},{"cell_type":"code","source":"def fieldsplit_parsed(listing,fields):\n    listingdict = {}\n    # print(fields)\n\n    postinglist = re.findall('[a-z]|[0-9]{1,1000}',listing)\n\n    # print(postinglist)\n    # print(listing)\n\n    for item in range(len(postinglist)):\n        cur = postinglist[item]\n        if cur in fields:\n            # print(cur)\n            listingdict[cur] = int(postinglist[item+1])\n    # print(listingdict)\n    return listingdict","metadata":{"tags":[],"cell_id":"00018-5034e53c-86fe-4863-8181-97fb88a953fe"},"outputs":[],"execution_count":553},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-fee89c12-6d95-4496-9d40-6c3734e49f9f"},"source":"def find_string_parsed(query_dict):\n    query_listings = {}\n    for category in query_dict.keys():\n        for token in query_dict[category]:\n            query_listing = search(token,f = open('./index/offset','r')).strip().split(' ')\n            # print(query_listing)\n            for i in range(len(query_listing)):\n                listing = query_listing[i]\n                listingdict = fieldsplit_parsed(listing,[category,'d'])\n                query_listing[i] = listingdict\n                # if len(listingdict)!=0:\n                #     print(listingdict)\n            \n            query_listings[token] = query_listing\n    return query_listings","execution_count":554,"outputs":[]},{"cell_type":"code","source":"def get_matches_parsed(query_listings):\n    results = defaultdict(lambda: 0)\n    docs = defaultdict(lambda: 1)\n    weight = {'t':100000, 'b':50, 'i':25, 'c':10, 'r':10, 'l':10}\n\n    for token in query_listings:\n        for query_listing in query_listings[token]:\n            # print(query_listing)\n            for key in query_listing:\n                if key == 'd':\n                    docs[query_listing['d']] *= 2\n                elif key != 'd':\n                    if key == 't':\n                        # print('weight,100000: ',query_listing['d'])\n                        results[query_listing['d']] += query_listing[key]*docs[query_listing['d']]*weight[key]\n\n    sorteddict = [(k, results[k]) for k in sorted(results, key=results.get, reverse=True)]\n    # print(sorteddict)\n    # print('\\n')\n    return sorteddict","metadata":{"tags":[],"cell_id":"00020-188605b3-dfba-4783-b3a5-664cdf021c08"},"outputs":[],"execution_count":555},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-72776583-c4ae-4d8b-8369-7797eca6cf35"},"source":"def run_parsed_query(query_dict):\n    flag = 0\n    docs_postlist = find_string_parsed(query_dict)\n    # print(docs_postlist)\n    matches = get_matches_parsed(docs_postlist)\n    chosen = rank(matches)\n    print_titles(chosen)\n    docs_intersect = []\n    # docs_postlist = {}\n    # for category in query_dict.keys():\n    #     for i in range(len(query_dict[category])):\n    #         doc_list = []\n    #         post = []\n    #         for j in index[query_dict[category][i]]:\n    #             if category == 't':\n    #                 splits = re.split('t', j)\n    #             if category == 'b':\n    #                 splits = re.split('b', j)\n    #             if category == 'i':\n    #                 splits = re.split('i', j)\n    #             if category == 'c':\n    #                 splits = re.split('c', j)\n    #             if category == 'l':\n    #                 splits = re.split('l', j)\n    #             if category == 'r':\n    #                 splits = re.split('r', j)\n    #             if len(splits) > 1:\n    #                 post.append(j)\n    #                 splits2 = re.split('d|b|i|l|r|t|c', j)\n    #                 doc_list.append(splits2[1])\n    #                 docs_postlist[category+':'+query_dict[category][i]]=j\n    #         docs_postlist[category+':'+query_dict[category][i]]=post\n    #         if flag == 0:\n    #             docs_intersect = doc_list\n    #             flag = 1\n    #         else:\n    #             docs_intersect = np.intersect1d(docs_intersect, doc_list)\n    return docs_intersect, docs_postlist","execution_count":556,"outputs":[]},{"cell_type":"markdown","source":"This is used to print the postlist of the queries","metadata":{"tags":[],"cell_id":"00012-ef013d8a-207f-4394-b4fe-e9fd866c3ff8"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-896725fa-1743-40c5-ac25-76f4532a870d"},"source":"def print_postlist(postlist, docs_ids):\n    for i in postlist.keys():\n        print(\"Postlist for\",i+\":\")\n        for j in postlist[i]:\n            print(j,end=' ')\n        print()\n        print()","execution_count":557,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-b9b90e0a-5bc5-4711-88b9-08fc2ff2c5fa","allow_embed":true},"source":"parsed, querytype = parse_query(QUERY)\ndocs_intersect = []\ndocs_postlist = {}\nif querytype == 0:\n    docs_intersect, docs_postlist = run_whole_query(parsed)\nelse:\n    docs_intersect, docs_postlist = run_parsed_query(parsed)\n# print_postlist(docs_postlist, docs_intersect)","execution_count":558,"outputs":[{"name":"stdout","text":"World cup\nAntisemitism in the Arab world\nMany-worlds interpretation\nHello world\nRNA world\nArab world\nFifth world\nFourth world\nSecond world war\nThe hello world program\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"4a18ff93-453c-4aaf-ba80-b9eca9a50653","deepnote_execution_queue":[]}}