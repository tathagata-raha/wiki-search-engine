{"cells":[{"cell_type":"markdown","source":"Importing the required libraries","metadata":{"tags":[],"cell_id":"00000-32c31f85-b354-440f-9bee-07bf17bcf85d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-2aa92249-39ce-457a-92d0-289dc06fb126"},"source":"import re\nimport pickle\nimport Stemmer\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\nimport sys\nimport os","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we have instantiated some global variables, initiated the stemmer and read the index file","metadata":{"tags":[],"cell_id":"00002-ab0f0b15-23a9-4a80-bd10-fad94b965e57"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-2c87213f-5fea-4387-affd-c95a73482b61"},"source":"STOP_DICT = {}\n# INPUT_FILE = '../enwiki-20200801-pages-articles-multistream1.xml-p1p30303'\nINDEX_DIR = 'index/'\nQUERY = 'World cup'\nif INDEX_DIR[-1]!='/':\n    INDEX_DIR+='/'\nSTOP_DICT = {}\nSTOP_FILE = ''\nif INDEX_DIR.split('/')[0] == '2018114017':\n    STOP_FILE = '2018114017/frequent.pickle'\nelse:\n    STOP_FILE = 'frequent.pickle'\nwith open(STOP_FILE, 'rb') as handle:\n    STOP_DICT = pickle.load(handle)\nhandle.close()\nstemmer = Stemmer.Stemmer('english')\n# f = open(INDEX_DIR+\"index\", \"r\")\n# index_string = f.read()\n# f.close()\n# temp = index_string.split('\\n')\n# index = defaultdict(list)\n# for i in temp:\n#     splits = i.split(':')\n#     index[splits[0]] = splits[1].split(' ')","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"The preprocess function is to process the text. It tokenizes the data, removes unnecessary \nnon-ASCII characters and punctuations, stem the words using pystemmer and remove stop words","metadata":{"tags":[],"cell_id":"00004-8864224f-1b91-432e-adc8-2e6c79926186"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-410a6b96-03eb-4a0b-af70-49b0de07d865"},"source":"def preprocess(text):\n    text = text.lower()\n    tokens = re.sub(r'[^A-Za-z0-9]+', r' ', text).split()\n    stemmed_stop_free = []\n    for token in tokens:\n        if token not in STOP_DICT:\n            stemmed_stop_free.append(stemmer.stemWord(token))\n    return stemmed_stop_free","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The parse_query function parses the function and determines if it is a field query or a normal query","metadata":{"tags":[],"cell_id":"00006-da3ca957-df73-4fea-8800-27c8a63d3ae3"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-00340e3c-ef61-4674-acf6-a132dd83098c"},"source":"def parse_query(query):\n    query_list = query.split(':')\n    if len(query_list) == 1:\n        return preprocess(query_list[0]) , 0\n    else:\n        query_dict = {}\n        # query_dict[query_list[0]] = ''\n        for i in range(1, len(query_list)-1):\n            query_dict[query_list[i-1][-1]] = preprocess(query_list[i][:-2])\n        query_dict[query_list[-2][-1]] = preprocess(query_list[-1])\n        return query_dict, 1","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"This function is used to handled to handle the normal query","metadata":{"tags":[],"cell_id":"00008-318fa041-8470-4717-9581-a2740acb36a7"}},{"cell_type":"code","source":"def search(token,delim,f):\n    # Compute filesize of open file sent to us\n    global indexsize\n    indexsize = 0\n\n    loc = token\n\n    if indexsize == 0:\n        indexsize = os.fstat(f.fileno()).st_size\n    hi = indexsize\n\n    lo=0\n    lookfor=loc\n    # print \"looking for: \",lookfor\n    while hi-lo > 1:\n        # Find midpoint and seek to it\n        loc = int((hi+lo)/2)\n        # print(\" hi = \",hi,\" lo = \",lo)\n        # print \"seek to: \",loc\n        f.seek(loc)\n        # Skip to beginning of line\n        while f.read(1) != '\\n':\n            pass\n\n        line = f.readline()\n        row=line.split(delim)\n\n        s = row[0]\n        if delim == ' ':\n            s=int(row[0])\n\n        # post=row[1]\n        # print(s)\n        # print(lookfor>s)\n\n        if lookfor == s:\n            # print(\"Found: \",lookfor)\n            if delim == ':':\n                offsetval = int(row[1])\n                ind = open('./index/index','r')\n                ind.seek(offsetval)\n                line = ind.readline()\n            \n                post = line.split(':')[1]\n            \n            elif delim == ' ':\n                post = ''\n                for i in range(1,len(row)):\n                    post += row[i]+' '\n\n            f.close()\n            # print(\"Found: \",lookfor)\n            return post  # Found\n\n        if lookfor < s:\n            # print('h1')\n            # Split into lower half\n            hi=loc\n\n        if lookfor > s:\n            # print('h2')\n            # Split into higher half\n            lo=loc\n        \n    # If not found\n    # print(\"Not Found: \",lookfor)\n    f.close()\n    return False","metadata":{"tags":[],"cell_id":"00009-7a57b33a-3c84-4e6e-99ec-32db27714564"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def fieldsplit(listing):\n    listingdict = {}\n    fields = ['t', 'b', 'i', 'c', 'r', 'l','d']\n    postinglist = re.findall('[a-z]|[0-9]{1,1000}',listing)\n    # print(postinglist)\n    for item in range(len(postinglist)):\n        cur = postinglist[item]\n        if cur in fields:\n            listingdict[cur] = int(postinglist[item+1])\n    # print(listingdict)\n    return listingdict","metadata":{"tags":[],"cell_id":"00010-98071860-ad73-4893-99ef-5f53bdaa8f25"},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def find_string_whole(tokens):\n    query_listings = {}\n    for token in tokens:\n        query_listing = search(token,':',f = open('index/offset','r')).strip().split(' ')\n        for i in range(len(query_listing)):\n            listing = query_listing[i]\n            listingdict = fieldsplit(listing)\n\n            query_listing[i] = listingdict\n        \n        query_listings[token] = query_listing\n    return query_listings\n    #     for i in range(len(query_listing)):\n    #         listing = query_listing[i]\n    #         listingdict = self.fieldsplit(listing)\n\n    #         query_listing[i] = listingdict\n        \n    #     query_listings[token] = query_listing\n\n    # return self.get_matches(query_listings)","metadata":{"tags":[],"cell_id":"00009-72940552-1ad9-4b2e-a10d-4c6b1a4ba17e"},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def rank(docs):\n    for key,val in docs[:10]:\n        # fp = open(\"./index/titles\",'r')\n        # print(key)\n        # docs = self.search(key,' ',fp)\n        print(key)\n        # fp.close()\n\n    print(\"\\n\\n\")\n    return","metadata":{"tags":[],"cell_id":"00012-82cd15de-b97f-4578-8d23-d6efa71f825b"},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def get_matches(query_listings):\n    results = defaultdict(lambda: 0)\n    docs = defaultdict(lambda: 1)\n    weight = {'t':100000, 'b':50, 'i':25, 'c':10, 'r':10, 'l':10}\n\n    for token in query_listings:\n        for query_listing in query_listings[token]:\n            # print(query_listing)\n            for key in query_listing:\n                if key == 'd':\n                    docs[query_listing['d']] *= 2\n                elif key != 'd':\n                    if key == 't':\n                        # print('weight,100000: ',query_listing['d'])\n                        results[query_listing['d']] += query_listing[key]*docs[query_listing['d']]*weight[key]\n\n    sorteddict = [(k, results[k]) for k in sorted(results, key=results.get, reverse=True)]\n    # print(sorteddict)\n    # print('\\n')\n    return sorteddict","metadata":{"tags":[],"cell_id":"00012-54e1dd12-00bd-48cc-a8c4-f0214abad0d2"},"outputs":[],"execution_count":66},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-8391eca1-d76e-4546-89b3-a668ef303b3c"},"source":"def run_whole_query(query):\n    docs_intersect = []\n    docs_postlist = find_string_whole(query)\n    matches = get_matches(docs_postlist)\n    rank(matches)\n    # print(matches)\n    # for i in range(len(query)):\n    #     doc_list = []\n    #     post = []\n    #     for j in index[query[i]]:\n    #         splits = re.split('d|b|i|l|r|t|c', j)\n    #         doc_list.append(splits[1])\n    #         post.append(j)\n    #     docs_postlist[query[i]]=post\n    #     if i == 0:\n    #         docs_intersect = doc_list\n    #     else:\n    #         docs_intersect = np.intersect1d(docs_intersect, doc_list)\n        \n    return docs_intersect, docs_postlist","execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"This function is used to handled to handle the field query","metadata":{"tags":[],"cell_id":"00010-edd2a198-23aa-4691-82ad-b7bf4e0fc306"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-72776583-c4ae-4d8b-8369-7797eca6cf35"},"source":"def run_parsed_query(query_dict):\n    flag = 0\n    docs_intersect = []\n    docs_postlist = {}\n    for category in query_dict.keys():\n        for i in range(len(query_dict[category])):\n            doc_list = []\n            post = []\n            for j in index[query_dict[category][i]]:\n                if category == 't':\n                    splits = re.split('t', j)\n                if category == 'b':\n                    splits = re.split('b', j)\n                if category == 'i':\n                    splits = re.split('i', j)\n                if category == 'c':\n                    splits = re.split('c', j)\n                if category == 'l':\n                    splits = re.split('l', j)\n                if category == 'r':\n                    splits = re.split('r', j)\n                if len(splits) > 1:\n                    post.append(j)\n                    splits2 = re.split('d|b|i|l|r|t|c', j)\n                    doc_list.append(splits2[1])\n                    docs_postlist[category+':'+query_dict[category][i]]=j\n            docs_postlist[category+':'+query_dict[category][i]]=post\n            if flag == 0:\n                docs_intersect = doc_list\n                flag = 1\n            else:\n                docs_intersect = np.intersect1d(docs_intersect, doc_list)\n    return docs_intersect, docs_postlist","execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"This is used to print the postlist of the queries","metadata":{"tags":[],"cell_id":"00012-ef013d8a-207f-4394-b4fe-e9fd866c3ff8"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-896725fa-1743-40c5-ac25-76f4532a870d"},"source":"def print_postlist(postlist, docs_ids):\n    for i in postlist.keys():\n        print(\"Postlist for\",i+\":\")\n        for j in postlist[i]:\n            print(j,end=' ')\n        print()\n        print()","execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-b9b90e0a-5bc5-4711-88b9-08fc2ff2c5fa","allow_embed":true},"source":"parsed, querytype = parse_query(QUERY)\ndocs_intersect = []\ndocs_postlist = {}\nif querytype == 0:\n    docs_intersect, docs_postlist = run_whole_query(parsed)\nelse:\n    docs_intersect, docs_postlist = run_parsed_query(parsed)\n# print_postlist(docs_postlist, docs_intersect)","execution_count":70,"outputs":[{"name":"stdout","text":"104133\n83456\n88258\n91082\n99150\n57454\n57991\n57994\n59817\n63950\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"00017-4f5ec9a6-0e01-4437-bd79-25908abbc661"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"4a18ff93-453c-4aaf-ba80-b9eca9a50653","deepnote_execution_queue":[]}}