{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-d1d060fd-0e01-47ac-8595-c89daabf3a7b"},"source":"!pip install pystemmer","execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pystemmer in /opt/venv/lib/python3.7/site-packages (2.0.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import the required libraries","metadata":{"tags":[],"cell_id":"00000-9f8b9aca-05a0-4d85-8867-3e0d3df4dd9c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-fed1eb12-fd0c-430d-8046-5747129c51de"},"source":"import xml.sax\nimport re\nfrom collections import defaultdict, OrderedDict\nimport Stemmer\nfrom os import path, listdir\nimport sys\nimport pickle\nimport time\nimport heapq","execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, we have instantiated some global variables and initiated the stemmer","metadata":{"tags":[],"cell_id":"00001-3bce8bfa-b904-4cbb-8922-cce137d27243"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-f5f3b1b9-716d-48f7-b982-2fd21a22de3c"},"source":"#Need to change these below\nINPUT_FILE = '../phase-2-data'\nOUTPUT = 'index/'\nINDEX_STAT = '​invertedindex_stat.txt​'\nTOTAL_TOKENS = 0\nTOTAL_INV_TOKENS = 0\nif OUTPUT[-1]!='/':\n    OUTPUT+='/'\nSTOP_DICT = {}\nSTOP_FILE = ''\nif OUTPUT.split('/')[0] == '2018114017':\n    STOP_FILE = '2018114017/frequent.pickle'\nelse:\n    STOP_FILE = 'frequent.pickle'\nwith open(STOP_FILE, 'rb') as handle:\n    STOP_DICT = pickle.load(handle)\nhandle.close()\nstemmer = Stemmer.Stemmer('english')\nstem_dict = {}\ntitle_dict = {}\nindexMap = defaultdict(list)\nfile_num = 0\npages = 1","execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"The preprocess function is to process the text. It tokenizes the data, removes unnecessary \nnon-ASCII characters and punctuations, stem the words using pystemmer and remove stop words","metadata":{"tags":[],"cell_id":"00004-a927679e-0594-4336-93eb-9fcdf66c090d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-77d60fc9-5fdb-487c-93a2-bea4b3920ddc"},"source":"def preprocess(text):\n    tokens = re.sub(r'[^A-Za-z0-9]+', r' ', text).split()\n    global TOTAL_TOKENS\n    TOTAL_TOKENS += len(tokens)\n    stemmed_stop_free = []\n    for token in tokens:\n        if token not in STOP_DICT:\n            temp_stem = ''\n            if token in stem_dict:\n                temp_stem = stem_dict[token]\n            else:\n                stem_dict[token] = stemmer.stemWord(token)\n                temp_stem = stem_dict[token]\n            stemmed_stop_free.append(temp_stem)\n            # stemmed_stop_free.append(stemmer.stemWord(token))\n    return stemmed_stop_free","execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The extract_under_ref function separates and preprocesses links, references and categories.\nThe extract_infobox_and_refs separates and preprocesses infobox contents and references.","metadata":{"tags":[],"cell_id":"00006-2fb2b3d1-fb78-4a11-889b-8e1affa93360"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-ef94cab1-0bf1-499e-b600-deb17a41c71c"},"source":"def extract_under_ref(splits):\n    if len(splits) == 1:\n        return [], [], []\n    else:\n        data = splits[1].split('\\n')\n        links = []\n        refs = []\n        categories = []\n        for line in data:\n            if re.match(r'\\*[\\ ]*\\[', line):\n                links.append(line)\n            if re.search(r'<ref', line):\n                refs.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n            if re.match(r'\\[\\[category', line):\n                categories.append(re.sub(r'\\[\\[category:(.*)\\]\\]', r'\\1', line))\n        # return links, refs, categories\n        return preprocess(' '.join(links)), preprocess(' '.join(refs)), preprocess(' '.join(categories))\n\ndef extract_infobox_and_refs(text):\n    data = text.split('\\n')\n    flag = 0\n    info = []\n    refs2 = []\n    for line in data:\n        for i in re.findall(\"{{cite.*title=.*}}\", line):\n            refs2.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n        if re.match(r'\\{\\{infobox', line):\n            flag = 1\n            info.append(re.sub(r'\\{\\{infobox(.*)', r'\\1', line))\n        elif flag == 1:\n            if line == '}}':\n                flag = 0\n                continue\n            info.append(line)\n    return preprocess(' '.join(info)), preprocess(' '.join(refs2))","execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"The split_page function splits a Wikipedia page into different parts like text, links, refs, body, categories","metadata":{"tags":[],"cell_id":"00008-df17f473-153c-4c12-8c8b-603ddcab54f0"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-062abdb9-6b77-4f5b-aea1-9615759ed52f"},"source":"def split_page( text):\n    # text = text.encode(\"ascii\", errors=\"ignore\").decode()\n    text = text.lower()\n    splits = re.split(r'== ?references.?.? ?==|== ?notes and references ?==',text)\n    # global pageCount\n    # global titlefile\n    # pageCount += 1\n    # if pageCount%1000 == 0:\n    #     print(pageCount)\n    if (len(splits)==1):\n        splits = re.split(r'== ?footnotes ?==', splits[0])\n    data = {}\n    data['links'], data['refs'], data['categories'] = extract_under_ref(splits)\n    data['text'] = preprocess(re.sub(r'\\{\\{.*\\}\\}', r' ', splits[0]))\n    data['infobox'], data['refs2'] = extract_infobox_and_refs(splits[0])\n    data['refs'] = data['refs'] + data['refs2']\n    return data","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The indexify function converts the document into inverted index","metadata":{"tags":[],"cell_id":"00010-7f10546b-7051-47b0-b51b-5d7bc59f5649"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-ff6a55da-b236-4d98-904d-f2cfd5bf1088"},"source":"def indexify(data):\n    global indexMap\n    totalFreq = defaultdict(lambda: 0)\n    inverted = {}\n    for i in ['title','text','infobox','categories','links','refs']:\n        d = defaultdict(lambda: 0)\n        for word in data[i]:\n            d[word] += 1\n            totalFreq[word] += 1\n        inverted[i] = d\n    for word in totalFreq.keys():\n        string = 'd'+str(data['id'])\n        for i in ['title','text','infobox','categories','links','refs']:\n            temp = inverted[i][word]\n            if temp:\n                if i != 'text':\n                    string += i[0] + str(temp)\n                else:\n                    string += 'b' + str(temp)\n        indexMap[word].append(string)","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The xml_handler class is used to parse he xml file and call all the above functions","metadata":{"tags":[],"cell_id":"00012-cc7b1497-9e91-4ff4-9aed-697890f5551c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-274404bf-44bc-49d2-8425-daedb7767ff3"},"source":"class xml_handler( xml.sax.ContentHandler ):\n    def re_init(self):\n        global pages\n        self.title = ''\n        self.text = ''\n        # self.hashed = 0\n        self.id = ''\n        self.pages += 1\n        pages += 1\n\n    def __init__(self, start_time, filenum):\n        global pages\n        self.CurrentData = ''\n        self.title = ''\n        self.text = ''\n        self.id = ''\n        self.link_len = 0\n        self.ref_len = 0\n        self.categories_len = 0\n        self.text_len = 0\n        self.title_len = 0\n        self.info_len = 0\n        self.pages = pages\n        self.start_time = start_time\n        self.filenum = filenum\n        # self.hashed = 0\n\n    # Call when an element starts\n    def startElement(self, tag, attributes):\n        self.CurrentData = tag\n\n    # Call when an elements ends\n    def endElement(self, tag):\n        if tag == 'page':\n            # wiki_page = Page( self.title, self.text, self.id )\n            # pages.append(wiki_page)\n            data = split_page(self.text)\n            data['title'] = preprocess(self.title)\n            data['id'] = self.pages\n            # if data['refs2'] == []:\n            #     print(self.pages)\n            # if data['id'] == 0:\n            #     print(data)\n            title_dict[self.pages] = self.title\n            indexify(data)\n            self.link_len += len(data['links'])\n            self.info_len += len(data['links'])\n            self.ref_len += len(data['refs'])\n            self.categories_len += len(data['categories'])\n            self.text_len += len(data['text'])\n            self.title_len += len(data['title'])\n            \n            self.re_init()\n            if self.pages %1000 == 0:\n                print(self.link_len, self.ref_len, self.categories_len, self.text_len)\n                print(\"Finished:\", self.pages, \"pages. Time elapsed:\",time.time() - self.start_time )\n            if self.pages % 50000 == 0:\n                print(\"Writing temporary index\")\n                store_index(str(self.filenum)+'_'+str(self.pages // 50000))\n    def endDocument(self):\n        if self.pages % 50000 != 0:\n            print(\"Writing temporary index\")\n            store_index(str(self.filenum)+'_'+str(self.pages // 50000 + 1))\n\n    # Call when a character is read\n    def characters(self, content):\n        if self.CurrentData == 'title':\n            self.title += content\n        if self.CurrentData == 'text':\n            self.text += content\n        # if self.CurrentData == 'id' and not self.hashed:\n        #     self.id = content\n        #     self.hashed = 1","execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"The store_index function stores the index and index stats in files","metadata":{"tags":[],"cell_id":"00014-3092b275-8f68-4043-9556-c928d1cfb69c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-f66ecd71-8075-4c08-8178-6ec58f5705df"},"source":"def store_index(st):\n    global indexMap\n    global title_dict\n    index_map_file = []\n    for key in sorted(indexMap.keys()):\n        string = key + ':' + ' '.join(indexMap[key])\n        index_map_file.append(string)\n    with open(OUTPUT+'index'+st+'.txt',\"w+\") as f:\n        f.write('\\n'.join(index_map_file))\n    with open(INDEX_STAT,\"w+\") as f:\n        f.write(str(TOTAL_TOKENS)+'\\n')\n        f.write(str(len(indexMap))+'\\n')\n    indexMap = defaultdict(list)\n    title_list = []\n    for key in title_dict.keys():\n        string = str(key) + ':' + title_dict[key]\n        title_list.append(string)\n    with open(OUTPUT+'titles',\"a+\") as f:\n        f.write(''.join(title_list))\n    title_dict = {}","execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The wiki_parse function calls the xml_handler class and starts parsing the xml file. It also measures the time taken to parse the files.","metadata":{"tags":[],"cell_id":"00016-d1808556-43ee-4db8-84c0-cb794dc0abc6"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-7241ab7b-016e-4910-9649-350c80040a68"},"source":"def wiki_parse():\n    print(\"Starting parser\")\n    file_num = 0\n    total_time = 0\n    for filename in listdir(INPUT_FILE):\n        parse_start_time = time.time()\n        xml_parser = xml.sax.make_parser()\n        xml_parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n        handler = xml_handler(parse_start_time, file_num)\n        xml_parser.setContentHandler(handler)\n        xml_parser.parse(INPUT_FILE+'/'+filename)\n        print(\"Parsing finished\")\n        # store_index(file_num)\n        # print(\"Dumping finished\")\n        time_taken = time.time() - parse_start_time\n        total_time += time_taken\n        print(\"Time taken: \",time_taken)\n        print(\"Time elapsed: \",total_time)\n        print(\"Pages: \",pages)\n        total_time += time_taken\n        indexMap = defaultdict(list)\n        file_num += 1","execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-2cc469a2-a52b-45fd-9d0b-cef84a0e2ae8","allow_embed":true},"source":"wiki_parse()","execution_count":27,"outputs":[{"name":"stdout","text":"Starting parser\n17313 28460 15025 810532\nFinished: 1000 pages. Time elapsed: 9.228018045425415\n31989 57373 24408 1711617\nFinished: 2000 pages. Time elapsed: 18.9800124168396\n60323 133724 38229 3808453\nFinished: 3000 pages. Time elapsed: 37.567622423172\n78506 178893 51251 5300404\nFinished: 4000 pages. Time elapsed: 50.573192834854126\n99754 230730 63085 6956289\nFinished: 5000 pages. Time elapsed: 65.46453714370728\n124720 274799 75055 8616949\nFinished: 6000 pages. Time elapsed: 79.94975996017456\n144094 315319 86267 9819227\nFinished: 7000 pages. Time elapsed: 90.82935309410095\n158826 341375 100396 10636739\nFinished: 8000 pages. Time elapsed: 99.7566077709198\n177828 374383 114373 11589584\nFinished: 9000 pages. Time elapsed: 109.83856081962585\n198937 409292 131054 12447584\nFinished: 10000 pages. Time elapsed: 120.18779683113098\n215008 441541 146975 13293078\nFinished: 11000 pages. Time elapsed: 129.56565189361572\n230518 498431 163383 14349151\nFinished: 12000 pages. Time elapsed: 140.44163703918457\n247492 555381 177599 15281793\nFinished: 13000 pages. Time elapsed: 151.02144145965576\n263275 632222 194770 16902189\nFinished: 14000 pages. Time elapsed: 166.25472855567932\n280520 702506 208083 18160570\nFinished: 15000 pages. Time elapsed: 178.9121172428131\n294071 752175 222572 19257849\nFinished: 16000 pages. Time elapsed: 191.31535005569458\n307250 810426 234880 20347293\nFinished: 17000 pages. Time elapsed: 202.6763036251068\n316698 862014 246075 21413521\nFinished: 18000 pages. Time elapsed: 214.25465178489685\n328826 909662 258337 22391881\nFinished: 19000 pages. Time elapsed: 225.95529532432556\n346106 967783 270395 23312092\nFinished: 20000 pages. Time elapsed: 235.43392825126648\n376648 1026006 282196 24254399\nFinished: 21000 pages. Time elapsed: 245.67385625839233\n393233 1095457 294359 25296285\nFinished: 22000 pages. Time elapsed: 256.3059961795807\n408572 1139612 306743 26308205\nFinished: 23000 pages. Time elapsed: 266.87695384025574\n417009 1154640 317320 27027970\nFinished: 24000 pages. Time elapsed: 274.62693977355957\n433216 1172887 325800 27723342\nFinished: 25000 pages. Time elapsed: 282.28923630714417\n438897 1193595 332935 28309412\nFinished: 26000 pages. Time elapsed: 289.52736234664917\n442160 1211141 339257 28869781\nFinished: 27000 pages. Time elapsed: 297.70137429237366\n447609 1251611 348373 29610652\nFinished: 28000 pages. Time elapsed: 305.9021189212799\n454546 1297162 358014 30465505\nFinished: 29000 pages. Time elapsed: 314.7862684726715\n466968 1343365 368134 31285584\nFinished: 30000 pages. Time elapsed: 323.6835470199585\n500485 1381408 384997 34740538\nFinished: 31000 pages. Time elapsed: 351.85083079338074\n518927 1442011 398545 35695989\nFinished: 32000 pages. Time elapsed: 362.023090839386\n536108 1495871 413126 36697953\nFinished: 33000 pages. Time elapsed: 372.7894515991211\n549392 1533475 427805 37616846\nFinished: 34000 pages. Time elapsed: 385.1523857116699\n558678 1576590 439690 38551722\nFinished: 35000 pages. Time elapsed: 395.273024559021\n575609 1616663 452459 39493377\nFinished: 36000 pages. Time elapsed: 405.46106457710266\n584131 1670733 462530 40308263\nFinished: 37000 pages. Time elapsed: 414.2272469997406\n592293 1726804 472941 41167684\nFinished: 38000 pages. Time elapsed: 423.2082793712616\n598276 1775540 484581 42020088\nFinished: 39000 pages. Time elapsed: 432.53916931152344\n608834 1817395 497808 42933257\nFinished: 40000 pages. Time elapsed: 442.85388135910034\n621897 1862358 509119 43879435\nFinished: 41000 pages. Time elapsed: 452.9113688468933\n634672 1894254 521882 45029528\nFinished: 42000 pages. Time elapsed: 464.1562476158142\n646748 1936135 534791 46013300\nFinished: 43000 pages. Time elapsed: 474.91631412506104\n652218 1960519 544177 46705469\nFinished: 44000 pages. Time elapsed: 482.57017707824707\n661140 1985453 552954 47440176\nFinished: 45000 pages. Time elapsed: 490.85424852371216\n668794 1997068 558777 47856590\nFinished: 46000 pages. Time elapsed: 499.54545187950134\n690434 2035293 577837 48952012\nFinished: 47000 pages. Time elapsed: 512.3607683181763\n708687 2069209 594925 49875371\nFinished: 48000 pages. Time elapsed: 523.2236647605896\n728778 2099762 610419 50853013\nFinished: 49000 pages. Time elapsed: 533.8625509738922\n748735 2136763 627425 51926940\nFinished: 50000 pages. Time elapsed: 545.6044306755066\nWriting temporary index\n767657 2171817 643912 52882203\nFinished: 51000 pages. Time elapsed: 575.0486350059509\n784451 2212339 660082 53886744\nFinished: 52000 pages. Time elapsed: 585.729241847992\n805339 2258610 679878 55026050\nFinished: 53000 pages. Time elapsed: 598.4493634700775\n824657 2290128 694709 55920581\nFinished: 54000 pages. Time elapsed: 609.2506189346313\n844148 2319220 710067 56724495\nFinished: 55000 pages. Time elapsed: 618.1730725765228\n859962 2345834 725862 57522584\nFinished: 56000 pages. Time elapsed: 627.0948424339294\n879108 2380318 742145 58544451\nFinished: 57000 pages. Time elapsed: 638.9290297031403\n897683 2415764 758132 59539604\nFinished: 58000 pages. Time elapsed: 650.2435712814331\n915808 2447661 775700 60820756\nFinished: 59000 pages. Time elapsed: 664.9455473423004\n937690 2483791 796370 61928217\nFinished: 60000 pages. Time elapsed: 677.0265529155731\n957728 2513843 813545 62984221\nFinished: 61000 pages. Time elapsed: 690.3166997432709\n976449 2552739 831842 64072863\nFinished: 62000 pages. Time elapsed: 702.2463171482086\n998613 2596721 852080 65167814\nFinished: 63000 pages. Time elapsed: 714.3801577091217\n1013869 2634602 866945 66299182\nFinished: 64000 pages. Time elapsed: 727.7607464790344\n1032955 2672437 885859 67664185\nFinished: 65000 pages. Time elapsed: 741.9644446372986\n1053597 2707046 900904 68745962\nFinished: 66000 pages. Time elapsed: 753.3358979225159\n1073993 2734648 917819 69582398\nFinished: 67000 pages. Time elapsed: 762.2810566425323\n1091766 2757292 930313 70287475\nFinished: 68000 pages. Time elapsed: 771.780816078186\n1116396 2796711 947198 71278102\nFinished: 69000 pages. Time elapsed: 782.9861841201782\n1139869 2847179 962505 72323473\nFinished: 70000 pages. Time elapsed: 794.5693607330322\n1161102 2879062 979655 73321912\nFinished: 71000 pages. Time elapsed: 804.0575261116028\n1180191 2907784 993668 74246253\nFinished: 72000 pages. Time elapsed: 816.0346567630768\n1198395 2932922 1007692 75106080\nFinished: 73000 pages. Time elapsed: 825.565985918045\n1216492 2960123 1020238 75904478\nFinished: 74000 pages. Time elapsed: 834.5349535942078\n1233927 2989461 1033847 76785949\nFinished: 75000 pages. Time elapsed: 844.6515784263611\n1250100 3014189 1046911 77620864\nFinished: 76000 pages. Time elapsed: 853.870278596878\n1261433 3031899 1056876 78292316\nFinished: 77000 pages. Time elapsed: 861.1387369632721\n1280800 3057010 1071297 79069526\nFinished: 78000 pages. Time elapsed: 869.6807403564453\n1297977 3084536 1086730 79917725\nFinished: 79000 pages. Time elapsed: 881.4450559616089\n1315605 3113625 1101313 80768502\nFinished: 80000 pages. Time elapsed: 890.7040531635284\n1332968 3144839 1115094 81639986\nFinished: 81000 pages. Time elapsed: 900.5634071826935\n1350154 3173032 1128503 82427563\nFinished: 82000 pages. Time elapsed: 909.2420830726624\nWriting temporary index\nParsing finished\nTime taken:  923.0962727069855\nTime elapsed:  923.0962727069855\nPages:  82291\n23665 56454 11596 1271146\nFinished: 83000 pages. Time elapsed: 16.406315326690674\n56460 114338 29152 2805279\nFinished: 84000 pages. Time elapsed: 34.10324454307556\n91050 185580 47772 4675125\nFinished: 85000 pages. Time elapsed: 55.950618505477905\n128755 260038 61533 6516333\nFinished: 86000 pages. Time elapsed: 80.50411438941956\n164259 319473 78517 8147620\nFinished: 87000 pages. Time elapsed: 99.81860280036926\n202153 397305 97743 9900868\nFinished: 88000 pages. Time elapsed: 119.85241413116455\n248512 454336 115469 11562857\nFinished: 89000 pages. Time elapsed: 140.30940651893616\n281653 526810 131679 13502916\nFinished: 90000 pages. Time elapsed: 161.5511496067047\n321081 589277 151625 15284539\nFinished: 91000 pages. Time elapsed: 183.86933875083923\n360278 667692 166275 17325282\nFinished: 92000 pages. Time elapsed: 208.42754435539246\n402433 747355 195359 19341173\nFinished: 93000 pages. Time elapsed: 234.69186854362488\n439175 809796 214645 21008544\nFinished: 94000 pages. Time elapsed: 254.7685031890869\n474542 881504 228889 22812134\nFinished: 95000 pages. Time elapsed: 277.28683042526245\n509277 956815 248627 24571796\nFinished: 96000 pages. Time elapsed: 299.9527590274811\n541961 1032145 263137 26515187\nFinished: 97000 pages. Time elapsed: 322.40643286705017\n578520 1100150 276698 28189904\nFinished: 98000 pages. Time elapsed: 342.7886519432068\n615285 1161400 292452 29787590\nFinished: 99000 pages. Time elapsed: 364.47723937034607\n654971 1237175 312041 31763834\nFinished: 100000 pages. Time elapsed: 388.1834077835083\nWriting temporary index\n693584 1307448 326537 33701960\nFinished: 101000 pages. Time elapsed: 422.6881356239319\n732622 1373632 343758 35493117\nFinished: 102000 pages. Time elapsed: 443.13019371032715\nWriting temporary index\nParsing finished\nTime taken:  446.3896119594574\nTime elapsed:  2292.5821573734283\nPages:  102088\n34749 64580 19884 1687563\nFinished: 103000 pages. Time elapsed: 19.58112359046936\n70336 151509 39978 3597177\nFinished: 104000 pages. Time elapsed: 45.94079375267029\n94275 207982 53281 6055857\nFinished: 105000 pages. Time elapsed: 70.65184569358826\n95324 215973 54779 6519394\nFinished: 106000 pages. Time elapsed: 75.38120341300964\n118416 262646 67297 7673417\nFinished: 107000 pages. Time elapsed: 88.70126461982727\n149640 316243 81912 9149436\nFinished: 108000 pages. Time elapsed: 104.84836745262146\n174522 358278 95070 10389277\nFinished: 109000 pages. Time elapsed: 119.1505913734436\n190384 374069 101892 11162239\nFinished: 110000 pages. Time elapsed: 127.31282234191895\n215087 414416 117556 12196951\nFinished: 111000 pages. Time elapsed: 139.1874279975891\n244606 465958 136798 13606606\nFinished: 112000 pages. Time elapsed: 156.16238403320312\n277124 515624 155583 15040664\nFinished: 113000 pages. Time elapsed: 172.63991141319275\n310504 556716 171929 16279204\nFinished: 114000 pages. Time elapsed: 186.14257645606995\n345604 603094 187799 17730186\nFinished: 115000 pages. Time elapsed: 203.69017457962036\n376037 652461 205625 19156086\nFinished: 116000 pages. Time elapsed: 219.7415895462036\n404121 704409 223696 20472504\nFinished: 117000 pages. Time elapsed: 234.99202632904053\n431167 759703 241746 21923875\nFinished: 118000 pages. Time elapsed: 251.0299904346466\n454561 800094 254981 23039425\nFinished: 119000 pages. Time elapsed: 265.81825709342957\n478721 844413 268186 24126895\nFinished: 120000 pages. Time elapsed: 277.4520363807678\n496940 887719 279531 25375478\nFinished: 121000 pages. Time elapsed: 290.32188272476196\n522338 927095 291640 26572831\nFinished: 122000 pages. Time elapsed: 302.93739223480225\n548019 970704 308950 27745569\nFinished: 123000 pages. Time elapsed: 315.93257546424866\n567063 1004532 324637 28844464\nFinished: 124000 pages. Time elapsed: 327.89254117012024\n593199 1055876 344650 30054380\nFinished: 125000 pages. Time elapsed: 344.7349696159363\n613687 1086983 356129 31039020\nFinished: 126000 pages. Time elapsed: 355.29222416877747\n641035 1129308 373084 32303838\nFinished: 127000 pages. Time elapsed: 369.48079466819763\n660270 1168822 387592 33407600\nFinished: 128000 pages. Time elapsed: 381.94502091407776\n680175 1202689 400357 34403134\nFinished: 129000 pages. Time elapsed: 393.3049235343933\n703156 1242012 416631 35600713\nFinished: 130000 pages. Time elapsed: 406.8691608905792\n727398 1288358 432156 36833421\nFinished: 131000 pages. Time elapsed: 423.4285502433777\n748958 1320530 449083 37758398\nFinished: 132000 pages. Time elapsed: 434.09737157821655\n770985 1353236 467228 38796691\nFinished: 133000 pages. Time elapsed: 445.7115466594696\n792980 1386124 480793 39716016\nFinished: 134000 pages. Time elapsed: 456.02192902565\n817879 1402861 489455 40455864\nFinished: 135000 pages. Time elapsed: 464.1647870540619\n842126 1434933 504523 41550837\nFinished: 136000 pages. Time elapsed: 476.3067727088928\n868727 1468664 517182 42690525\nFinished: 137000 pages. Time elapsed: 488.5019462108612\n893973 1493456 526918 43471943\nFinished: 138000 pages. Time elapsed: 497.88794207572937\n909084 1511870 537253 44191313\nFinished: 139000 pages. Time elapsed: 506.36684107780457\n921463 1527615 545496 44702434\nFinished: 140000 pages. Time elapsed: 512.1909046173096\nWriting temporary index\nParsing finished\nTime taken:  538.9499990940094\nTime elapsed:  3277.921768426895\nPages:  140867\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00019-95d3375a-92ed-4c36-ae4b-cdf57aa31b0c"},"source":"def merge_index():\n    global pages\n    file_pointers = []\n    for filename in listdir(OUTPUT):\n        if filename.endswith('.txt'):\n            print(OUTPUT+filename)\n            file_pointers.append(open(OUTPUT+filename, 'r'))\n    # ind = open('index/index','w+')\n    # off = open('index/offset','w+')\n    file_pointers_temp = file_pointers[0:40]\n    last_file_count = 40\n    file_pointers_flag = 1\n    loop_count = 0\n    while file_pointers_flag:\n        print(last_file_count)\n        wordpostings = defaultdict(lambda: [])\n        words = {}\n        heap = []\n        wordfilemap = defaultdict(lambda: [])\n        curline = {}\n        finishflag = 1\n        flag = 0\n        filecomplete = [0 for i in range(len(file_pointers_temp))]\n        ind = open(OUTPUT+'index_temp'+str(loop_count),'w+')\n        off = open(OUTPUT+'offset_temp'+str(loop_count),'w+')\n        for i in range(len(file_pointers_temp)):\n            curline[i] = file_pointers_temp[i].readline().strip()\n            # print(curline[i])\n            word = curline[i].split(':')[0]\n            wordfilemap[word].append(i)\n            wordpostings[word] += curline[i].split(':')[1].split(\" \")\n            if word not in heap:\n                heapq.heappush(heap,word)\n        while (finishflag):\n            minword = heapq.heappop(heap)\n            string = minword + ':' + str(ind.tell())\n            string = string.strip() + '\\n'\n            off.write(string)\n            del string\n            string = minword + \":\" + \" \".join(wordpostings[minword]) + \"\\n\"\n            ind.write(string)\n            del string\n            filenum = wordfilemap[minword]\n            # print(wordfilemap)\n            wordfilemap.pop(minword)\n            wordpostings.pop(minword)\n            for num in filenum:\n                nextline = file_pointers_temp[num].readline().strip()\n                if nextline == '':\n                    filecomplete[num] = 1\n                else:\n                    newword = nextline.split(':')[0]\n                    wordpostings[newword] += nextline.split(':')[1].split(\" \")\n                    # print(wordfilemap[newword])\n                    if not wordfilemap[newword]:\n                        heapq.heappush(heap,newword)\n                        wordfilemap[newword].append(num)\n                    else:\n                        wordfilemap[newword].append(num)\n            for i in range(len(file_pointers_temp)):\n                flag = filecomplete[i] + flag\n            flag = int(flag/(len(file_pointers_temp)))\n            if flag==1:\n                finishflag = 0\n        for i in range(len(file_pointers_temp)):\n            file_pointers_temp[i].close()\n        ind.close()\n        off.close()\n        if file_pointers_flag==2:\n            file_pointers_flag = 0\n        elif last_file_count+39 >= len(file_pointers):\n            file_pointers_temp = file_pointers[last_file_count:]\n            file_pointers_flag = 2\n        else:\n            file_pointers_temp = file_pointers[last_file_count:last_file_count+39]\n        file_pointers_temp.append(open('index_temp/index_temp'+str(loop_count),'r'))\n        last_file_count = last_file_count + 39\n        loop_count += 1\n        wordpostings = None\n        words = None\n        heap = None\n        wordfilemap = None\n        curline = None\n    return","execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-f678ddd4-3c0a-4885-a00c-1fcac9cb2d59","allow_embed":false},"source":"def merge_index():\n    global pages\n    file_pointers = []\n    wordpostings = defaultdict(lambda: [])\n    words = {}\n    heap = []\n    wordfilemap = defaultdict(lambda: [])\n    curline = {}\n    finishflag = 1\n    flag = 0\n    for filename in listdir(OUTPUT):\n        if filename.endswith('.txt'):\n            print(OUTPUT+filename)\n            file_pointers.append(open(OUTPUT+filename, 'r'))\n    filecomplete = [0 for i in range(len(file_pointers))]\n    ind = open('index/index','w+')\n    off = open('index/offset','w+')\n    for i in range(len(file_pointers)):\n        curline[i] = file_pointers[i].readline().strip()\n        # print(curline[i])\n        word = curline[i].split(':')[0]\n        wordfilemap[word].append(i)\n        wordpostings[word] += curline[i].split(':')[1].split(\" \")\n        if word not in heap:\n            heapq.heappush(heap,word)\n\n    while (finishflag):\n        minword = heapq.heappop(heap)\n        string = minword + ':' + str(ind.tell())\n        string = string.strip() + '\\n'\n        off.write(string)\n        del string\n        string = minword + \":\" + \" \".join(wordpostings[minword]) + \"\\n\"\n        ind.write(string)\n        del string\n        filenum = wordfilemap[minword]\n        # print(wordfilemap)\n        wordfilemap.pop(minword)\n        for num in filenum:\n            nextline = file_pointers[num].readline().strip()\n            if nextline == '':\n                filecomplete[num] = 1\n            else:\n                newword = nextline.split(':')[0]\n                wordpostings[newword] += nextline.split(':')[1].split(\" \")\n                # print(wordfilemap[newword])\n                if not wordfilemap[newword]:\n                    heapq.heappush(heap,newword)\n                    wordfilemap[newword].append(num)\n                else:\n                    wordfilemap[newword].append(num)\n        for i in range(len(file_pointers)):\n            flag = filecomplete[i] + flag\n        flag = int(flag/(len(file_pointers)))\n        if flag==1:\n            finishflag = 0\n    for i in range(len(file_pointers)):\n        file_pointers[i].close()\n    ind.close()\n    off.close()\n\n    return","execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-304ff873-db5d-4a3b-84e7-a8f3127f2609"},"source":"merge_index()","execution_count":33,"outputs":[{"name":"stdout","text":"index/index2_3.txt\nindex/index1_2.txt\nindex/index0_1.txt\nindex/index0_2.txt\nindex/index1_3.txt\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-f7e3fa8a-3412-4c9e-a67b-ade995759425"},"source":"len(offset.keys())","execution_count":31,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'offset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-aba4273b9dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'offset' is not defined"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-b88df100-46a4-4e9b-98a5-f5f2fc2bc44c","allow_embed":true},"source":"# count = 0\n# t_list = []\n# for i in indexMap['egypt']:\n#     splits = re.split('c',i)\n#     if(len(splits) > 1):\n#         t_list.append(preprocess(title_dict[int(re.split('d|b|i',splits[0])[1])].lower()))\n#         print(int(re.split('d|b|i',splits[0])[1]))\n# print(count)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-80a915eb-3abb-42c0-b9de-9720fd6cfd07"},"source":"# for i in range(len(title_dict)):\n#     if preprocess(title_dict[i].lower()) == ['kellogg', 'briand', 'pact']:\n#         print(i)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-846a2d9a-2e9e-49b3-a4cd-81730ee5f6f7","allow_embed":true},"source":"# string = 'Ancient Egypt; Abydos, Egypt; Amasis II; Ammonius Saccas; Ababda people; Aswan; Abbas II of Egypt; Ambrose of Alexandria; Alexandria; Athanasius of Alexandria; Anthony the Great; Basel Convention; Battle of the Nile; Battle of Actium; Convention on Biological Diversity; CITES; Environmental Modification Convention; Cairo; Clement of Alexandria; Cyril of Alexandria; Coptic Orthodox Church of Alexandria; Duke Nukem 3D; Diophantus; Geography of Egypt; Demographics of Egypt; Politics of Egypt; Economy of Egypt; Telecommunications in Egypt; Transport in Egypt; Egyptian Armed Forces; Foreign relations of Egypt; Book of Exodus; First Battle of El Alamein; Go Down Moses; Great Pyramid of Giza; Great Rift Valley; Herodotus; History of Egypt; International Tropical Timber Agreement, 1983; International Tropical Timber Agreement, 1994; Imhotep; Kyoto Protocol; Kellogg–Briand Pact; Lighthouse of Alexandria; Library of Alexandria; Maimonides; Montreal Protocol; Mark Antony; Metre Convention; Muslim Brotherhood; Munich massacre; Nile; Treaty on the Non-Proliferation of Nuclear Weapons; Ozymandias; Origen; Pachomius the Great; Prospero Alpini; Pompey; Ptolemy; Ptolemaic dynasty; Palestine Liberation Organization; Red Sea; Rosetta Stone; Return to Castle Wolfenstein; Saladin; Sahara desert (ecoregion); Sinai Peninsula; Stargate (film); Saluki; Suez Canal; Six-Day War; Second Battle of El Alamein; Tax'\n# gaurang = string.split(';')\n# g_list = []\n# for i in gaurang:\n#     g_list.append(preprocess(i.lower()))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-5d589e68-8336-4a45-9369-0d4f742d8a04"},"source":"# for element in g_list:\n#     if element not in t_list:\n#         print(element)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-d6745983-af18-482e-9734-04cf95fa9751","allow_embed":true},"source":"# indexMap['egypt']","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-a255a383-0f92-46eb-9603-5a5e55eb58a0","allow_embed":true},"source":"# for i in ['2510', '12182', '13075', '19205']:\n#     print(title_dict[int(i)], end='; ')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-9aa445a6-a3b1-4018-9818-51516f10bbcb"},"source":"# line = '{{cite news|last=Rendell|first=Ruth|authorlink=Ruth Rendell|title=A most serious and extraordinary problem |url=https://www.theguardian.com/books/2008/sep/13/arthurconandoyle.crime|newspaper=[[The Guardian]]|date= 12 September 2008|accessdate=8 December 2018}}'\n# re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-7a2e3e6a-29c7-4274-b6a0-4a1d72896215"},"source":"# TOTAL_TOKENS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00019-65fdb28a-badc-44f7-8723-b34b5e4b74e8"},"source":"# len(indexMap)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00020-9d233416-69a7-487e-8d8d-659742e59c81"},"source":"filename","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00029-e6914565-a60c-4ee8-a7fb-b495bc0c3312"},"source":"help(xml.sax.handler)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00030-2f1c19f5-1c95-4007-8faa-6681a67a10dd"},"source":"del stem_dict","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00031-8ade9cbe-74a0-4574-9759-5811b2e73eee"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"0531998c-a2c2-4a55-b537-217d6cd2d969","deepnote_execution_queue":[],"deepnote_published_id":"a9dd1f37-f823-46dc-a10b-3c6a88567883"}}