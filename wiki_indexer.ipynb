{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-fed1eb12-fd0c-430d-8046-5747129c51de"},"source":"import xml.sax\nimport re\nfrom collections import defaultdict, OrderedDict\nimport Stemmer\nfrom os import path\nimport sys\nimport pickle\nimport time","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Need to change these below","metadata":{"tags":[],"cell_id":"00001-3bce8bfa-b904-4cbb-8922-cce137d27243"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-f5f3b1b9-716d-48f7-b982-2fd21a22de3c"},"source":"INPUT_FILE = '../enwiki-20200801-pages-articles-multistream1.xml-p1p30303'\nOUTPUT = 'index/'\nif OUTPUT[-1]!='/':\n    OUTPUT+='/'\nSTOP_DICT = {}\nSTOP_FILE = ''\nif OUTPUT.split('/')[0] == '2018114017':\n    STOP_FILE = '2018114017/stopwords.pickle'\nelse:\n    STOP_FILE = 'stopwords.pickle'\nwith open(STOP_FILE, 'rb') as handle:\n    STOP_DICT = pickle.load(handle)\nhandle.close()\nstemmer = Stemmer.Stemmer('english')\nstem_dict = {}\ntitle_dict = {}\nindexMap = defaultdict(list)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-77d60fc9-5fdb-487c-93a2-bea4b3920ddc"},"source":"def preprocess(text):\n    tokens = re.sub(r'[^A-Za-z0-9]+', r' ', text).split()\n    stemmed_stop_free = []\n    for token in tokens:\n        if token not in STOP_DICT:\n            if token in stem_dict:\n                stemmed_stop_free.append(stem_dict[token])\n            else:\n                stem_dict[token] = stemmer.stemWord(token)\n                stemmed_stop_free.append(stem_dict[token])\n            # stemmed_stop_free.append(stemmer.stemWord(token))\n    return stemmed_stop_free","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-ef94cab1-0bf1-499e-b600-deb17a41c71c"},"source":"def extract_under_ref(splits):\n    if len(splits) == 1:\n        return [], [], []\n    else:\n        data = splits[1].split('\\n')\n        links = []\n        refs = []\n        categories = []\n        for line in data:\n            if re.match(r'\\*[\\ ]*\\[', line):\n                links.append(line)\n            if re.search(r'<ref', line):\n                refs.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n            if re.match(r'\\[\\[category', line):\n                categories.append(re.sub(r'\\[\\[category:(.*)\\]\\]', r'\\1', line))\n        # return links, refs, categories\n        return preprocess(' '.join(links)), preprocess(' '.join(refs)), preprocess(' '.join(categories))\n\ndef extract_infobox_and_refs(text):\n    data = text.split('\\n')\n    flag = 0\n    info = []\n    refs2 = []\n    for line in data:\n        if re.match(r'\\{\\{infobox', line):\n            flag = 1\n            info.append(re.sub(r'\\{\\{infobox(.*)', r'\\1', line))\n        elif flag == 1:\n            if line == '}}':\n                flag = 0\n                continue\n            info.append(line)\n        for i in re.findall(\"{{cite.*title=.*}}\", line):\n            refs2.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n    return preprocess(' '.join(info)), preprocess(' '.join(refs2))\n# def getReferences(self, text):\n\n#     data = text.split('\\n')\n#     refs = []\n#     for line in data:\n#         if re.search(r'<ref', line):\n#             refs.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n\n#     return self.process(' '.join(refs))\n\n\n# def getCategories(self, text):\n    \n#     data = text.split('\\n')\n#     categories = []\n#     for line in data:\n#         if re.match(r'\\[\\[category', line):\n#             categories.append(re.sub(r'\\[\\[category:(.*)\\]\\]', r'\\1', line))\n    \n#     return self.process(' '.join(categories))\n\n\n# def getExternalLinks(self, text):\n    \n#     data = text.split('\\n')\n#     links = []\n#     for line in data:\n#         if re.match(r'\\*[\\ ]*\\[', line):\n#             links.append(line)\n    \n#     return self.process(' '.join(links))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-062abdb9-6b77-4f5b-aea1-9615759ed52f"},"source":"def split_page( text):\n    # text = text.encode(\"ascii\", errors=\"ignore\").decode()\n    text = text.lower()\n    splits = re.split(r'== ?references.?.? ?==|== ?notes and references ?==',text)\n    # global pageCount\n    # global titlefile\n    # pageCount += 1\n    # if pageCount%1000 == 0:\n    #     print(pageCount)\n    if (len(splits)==1):\n        splits = re.split(r'== ?footnotes ?==', splits[0])\n    data = {}\n    data['links'], data['refs'], data['categories'] = extract_under_ref(splits)\n    data['text'] = preprocess(re.sub(r'\\{\\{.*\\}\\}', r' ', splits[0]))\n    data['infobox'], data['refs2'] = extract_infobox_and_refs(splits[0])\n    data['refs'] = data['refs'] + data['refs2']\n    return data\n    # data['infobox']\n\n    # print(self.title)\n    # titlefile = open('./index/titles','a')\n    # string = str(self.id)+' '+self.title\n    # string = string.strip().encode(\"ascii\", errors=\"ignore\").decode() + '\\n'\n    # titlefile.write(string)\n    # titlefile.close()\n\n    # self.infobox = self.getInfobox(data[0])\n    # self.text = self.getBody(data[0])\n    # self.title = self.getTitle(self.title)\n\n    # print(self.title)\n    # titlefile.write(str(self.id)+' '+self.title)\n\n    # return self","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-ff6a55da-b236-4d98-904d-f2cfd5bf1088"},"source":"def indexify(data):\n    # global pageCount\n    # global fileCount\n    global indexMap\n    # global offset\n    # global dictID\n    # global filemap\n\n    # ID = pageCount\n    totalFreq = defaultdict(lambda: 0)\n\n    d = defaultdict(lambda: 0)\n    for word in data['title']:\n        d[word] += 1\n        totalFreq[word] += 1\n    title = d\n    \n    d = defaultdict(lambda: 0)\n    for word in data['text']:\n        d[word] += 1\n        totalFreq[word] += 1\n    body = d\n\n    d = defaultdict(lambda: 0)\n    for word in data['infobox']:\n        d[word] += 1\n        totalFreq[word] += 1\n    info = d\n\n    d = defaultdict(lambda: 0)\n    for word in data['categories']:\n        d[word] += 1\n        totalFreq[word] += 1\n    categories = d\n    \n    d = defaultdict(lambda: 0)\n    for word in data['links']:\n        d[word] += 1\n        totalFreq[word] += 1\n    links = d\n    \n    d = defaultdict(lambda: 0)\n    for word in data['refs']:\n        d[word] += 1\n        totalFreq[word] += 1\n    references = d\n\n    for word in totalFreq.keys():\n        t = title[word]\n        b = body[word]\n        i = info[word]\n        c = categories[word]\n        l = links[word]\n        r = references[word]\n        string = 'd'+str(data['id'])\n        if t:\n            string += 't' + str(t)\n        if b:\n            string += 'b' + str(b)\n        if i:\n            string += 'i' + str(i)\n        if c:\n            string += 'c' + str(c)\n        if l:\n            string += 'l' + str(l)\n        if r:\n            string += 'r' + str(r)\n    \n        indexMap[word].append(string)\n    \n    # print(indexMap)\n\n    # if pageCount%20000 == 0:\n    \n\n    #     indexMap = defaultdict(list)\n    #     dictID = {}\n    #     orderedMap = []\n    #     fileCount += 1","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-274404bf-44bc-49d2-8425-daedb7767ff3"},"source":"class xml_handler( xml.sax.ContentHandler ):\n    def re_init(self):\n        self.title = ''\n        self.text = ''\n        # self.hashed = 0\n        self.id = ''\n        self.pages += 1\n    def __init__(self, start_time):\n        self.CurrentData = ''\n        self.title = ''\n        self.text = ''\n        self.id = ''\n        self.link_len = 0\n        self.ref_len = 0\n        self.categories_len = 0\n        self.text_len = 0\n        self.pages = 0\n        self.start_time = start_time\n        # self.hashed = 0\n\n    # Call when an element starts\n    def startElement(self, tag, attributes):\n        self.CurrentData = tag\n\n    # Call when an elements ends\n    def endElement(self, tag):\n        if tag == 'page':\n            # wiki_page = Page( self.title, self.text, self.id )\n            # pages.append(wiki_page)\n            data = split_page(self.text)\n            data['title'] = preprocess(self.title)\n            data['id'] = self.pages\n            # if data['refs2'] == []:\n            #     print(self.pages)\n            # if data['id'] == 0:\n            #     print(data)\n            title_dict[self.pages] = self.title\n            indexify(data)\n            self.link_len += len(data['links'])\n            self.ref_len += len(data['refs'])\n            self.categories_len += len(data['categories'])\n            self.text_len += len(data['text'])\n            self.re_init()\n            if self.pages %1000 == 0:\n                print(self.link_len, self.ref_len, self.categories_len, self.text_len)\n                print(\"Finished:\", self.pages, \"pages. Time elapsed:\",time.time() - self.start_time )\n\n    # Call when a character is read\n    def characters(self, content):\n        if self.CurrentData == 'title':\n            self.title += content\n        if self.CurrentData == 'text':\n            self.text += content\n        # if self.CurrentData == 'id' and not self.hashed:\n        #     self.id = content\n        #     self.hashed = 1","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-f66ecd71-8075-4c08-8178-6ec58f5705df"},"source":"def store_index():\n    orderedMap = []\n    for key in sorted(indexMap.keys()):\n        string = key + ':'\n        posting_list = indexMap[key]\n        string += ' '.join(posting_list)\n        orderedMap.append(string)\n    with open(OUTPUT+'index.txt',\"w+\") as f:\n        f.write('\\n'.join(orderedMap))\n","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-7241ab7b-016e-4910-9649-350c80040a68"},"source":"def wiki_parse(xml_file):\n    print(\"Starting parser\")\n    parse_start_time = time.time()\n    xml_parser = xml.sax.make_parser()\n    xml_parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n    handler = xml_handler(parse_start_time)\n    xml_parser.setContentHandler(handler)\n    xml_parser.parse(xml_file)\n    print(\"Parsing finished\")\n    parse_end_time = time.time()\n    print(\"Total time taken: \",parse_end_time - parse_start_time)\n    store_index()\n    # pickle_out = open(\"index.pickle\",\"wb\")\n    # pickle.dump(indexMap, pickle_out)\n    # pickle_out.close()\n    print(\"Dumping finished\")\n    print(\"Time taken: \",time.time() - parse_end_time)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-2cc469a2-a52b-45fd-9d0b-cef84a0e2ae8","allow_embed":true},"source":"wiki_parse(INPUT_FILE)","outputs":[{"name":"stdout","text":"Starting parser\n31095 73292 17649 1678599\nFinished: 1000 pages. Time elapsed: 13.649773597717285\n66021 136016 34420 3280991\nFinished: 2000 pages. Time elapsed: 23.817848682403564\n102451 208218 53932 5226207\nFinished: 3000 pages. Time elapsed: 35.37585115432739\n139159 273622 67518 6936562\nFinished: 4000 pages. Time elapsed: 45.96837639808655\n174984 339189 83637 8612632\nFinished: 5000 pages. Time elapsed: 55.687307834625244\n213935 417787 101593 10479628\nFinished: 6000 pages. Time elapsed: 66.50373840332031\n259508 476956 119599 12156359\nFinished: 7000 pages. Time elapsed: 75.35969042778015\n293916 546615 137999 14019600\nFinished: 8000 pages. Time elapsed: 85.77768611907959\n334978 611119 156669 15993979\nFinished: 9000 pages. Time elapsed: 96.26197052001953\n371413 687776 170986 17823195\nFinished: 10000 pages. Time elapsed: 107.40301632881165\n413753 767830 202409 19846581\nFinished: 11000 pages. Time elapsed: 118.15333843231201\n451547 825757 219276 21448902\nFinished: 12000 pages. Time elapsed: 127.44615292549133\n485158 903956 234418 23372693\nFinished: 13000 pages. Time elapsed: 137.9499547481537\n519286 983323 252787 25207774\nFinished: 14000 pages. Time elapsed: 147.90649461746216\n553184 1054295 267274 26986443\nFinished: 15000 pages. Time elapsed: 158.12905025482178\n589814 1117205 282196 28648969\nFinished: 16000 pages. Time elapsed: 167.28613209724426\n627570 1186328 298671 30472009\nFinished: 17000 pages. Time elapsed: 177.65977001190186\n663684 1262382 316117 32344169\nFinished: 18000 pages. Time elapsed: 189.1535713672638\n706684 1326017 331738 34196429\nFinished: 19000 pages. Time elapsed: 198.60639452934265\nParsing finished\nTotal time taken:  206.1571807861328\nDumping finished\nTime taken:  3.578906297683716\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-b88df100-46a4-4e9b-98a5-f5f2fc2bc44c","allow_embed":true},"source":"# count = 0\n# t_list = []\n# for i in indexMap['egypt']:\n#     splits = re.split('c',i)\n#     if(len(splits) > 1):\n#         t_list.append(preprocess(title_dict[int(re.split('d|b|i',splits[0])[1])].lower()))\n#         print(int(re.split('d|b|i',splits[0])[1]))\n# print(count)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-80a915eb-3abb-42c0-b9de-9720fd6cfd07"},"source":"# for i in range(len(title_dict)):\n#     if preprocess(title_dict[i].lower()) == ['kellogg', 'briand', 'pact']:\n#         print(i)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-846a2d9a-2e9e-49b3-a4cd-81730ee5f6f7","allow_embed":true},"source":"# string = 'Ancient Egypt; Abydos, Egypt; Amasis II; Ammonius Saccas; Ababda people; Aswan; Abbas II of Egypt; Ambrose of Alexandria; Alexandria; Athanasius of Alexandria; Anthony the Great; Basel Convention; Battle of the Nile; Battle of Actium; Convention on Biological Diversity; CITES; Environmental Modification Convention; Cairo; Clement of Alexandria; Cyril of Alexandria; Coptic Orthodox Church of Alexandria; Duke Nukem 3D; Diophantus; Geography of Egypt; Demographics of Egypt; Politics of Egypt; Economy of Egypt; Telecommunications in Egypt; Transport in Egypt; Egyptian Armed Forces; Foreign relations of Egypt; Book of Exodus; First Battle of El Alamein; Go Down Moses; Great Pyramid of Giza; Great Rift Valley; Herodotus; History of Egypt; International Tropical Timber Agreement, 1983; International Tropical Timber Agreement, 1994; Imhotep; Kyoto Protocol; Kelloggâ€“Briand Pact; Lighthouse of Alexandria; Library of Alexandria; Maimonides; Montreal Protocol; Mark Antony; Metre Convention; Muslim Brotherhood; Munich massacre; Nile; Treaty on the Non-Proliferation of Nuclear Weapons; Ozymandias; Origen; Pachomius the Great; Prospero Alpini; Pompey; Ptolemy; Ptolemaic dynasty; Palestine Liberation Organization; Red Sea; Rosetta Stone; Return to Castle Wolfenstein; Saladin; Sahara desert (ecoregion); Sinai Peninsula; Stargate (film); Saluki; Suez Canal; Six-Day War; Second Battle of El Alamein; Tax'\n# gaurang = string.split(';')\n# g_list = []\n# for i in gaurang:\n#     g_list.append(preprocess(i.lower()))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-5d589e68-8336-4a45-9369-0d4f742d8a04"},"source":"# for element in g_list:\n#     if element not in t_list:\n#         print(element)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-d6745983-af18-482e-9734-04cf95fa9751","allow_embed":true},"source":"# indexMap['egypt']","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-a255a383-0f92-46eb-9603-5a5e55eb58a0","allow_embed":true},"source":"# for i in ['2510', '12182', '13075', '19205']:\n#     print(title_dict[int(i)], end='; ')","outputs":[{"name":"stdout","text":"BBC Radio 1\n    ; Morton Downey Jr.\n    ; Musical ensemble\n    ; Syracuse University\n    ; ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00016-9aa445a6-a3b1-4018-9818-51516f10bbcb"},"source":"# line = '{{cite news|last=Rendell|first=Ruth|authorlink=Ruth Rendell|title=A most serious and extraordinary problem |url=https://www.theguardian.com/books/2008/sep/13/arthurconandoyle.crime|newspaper=[[The Guardian]]|date= 12 September 2008|accessdate=8 December 2018}}'\n# re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line)","outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"00018-7a2e3e6a-29c7-4274-b6a0-4a1d72896215"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"0531998c-a2c2-4a55-b537-217d6cd2d969","deepnote_execution_queue":[],"deepnote_published_id":"a9dd1f37-f823-46dc-a10b-3c6a88567883"}}